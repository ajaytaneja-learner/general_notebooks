{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gorilla Hosted - Try it out in less than 60s üöÄ\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ShishirPatil/gorilla)  [![arXiv](https://img.shields.io/badge/arXiv-2305.15334-<COLOR>.svg?style=flat-square)](https://arxiv.org/abs/2305.15334)   [![Discord](https://img.shields.io/discord/1111172801899012102?label=Discord&logo=discord&logoColor=green&style=flat-square)](https://discord.gg/SwTyuTAxX3)  [![Twitter](https://img.shields.io/twitter/url?url=https://twitter.com/shishirpatil_/status/1661780076277678082)](https://twitter.com/shishirpatil_/status/1661780076277678082)\n",
        "\n",
        "Play around with Gorilla! Here, we host the Gorilla zero-shot models, so you can try it out! This is compatible with the OpenAI chat completion API - plug and play!\n",
        "\n",
        "üü¢ Now with Apache-2.0! Gorilla is commercially usable with no obligations üöÄ\n",
        "\n",
        "We are happy to launch all three models: `gorilla-7b-hf-v1` which chooses from 925 Hugging Face APIs 0-shot, `gorilla-7b-th-v0` for 94 (exhaustive) Tensor Hub APIs 0-shot, `gorilla-7b-tf-v0` for 626 (exhaustive) Tensorflow Hub APIs 0-shot. `gorilla-mpt-7b-hf-v0` and `gorilla-falcon-7b-hf-v0`are two Apache-2.0 licensed models for Hugging Face APIs. We have a hosted end-point for `gorilla-mpt-7b-hf-v0` in this colab, and are in the process of adding `gorilla-falcon-7b-hf-v0` soon! In spirit of openess, we do not filter, nor carry out any post processing either to the prompt nor response. We will release the combined {HF+TF+TH} model which also has generic chat capability slowly to accomodate server demand.\n",
        "\n",
        "üíÉ If you want to use Gorilla or build on top of it! Feel absolutely free to do so - we believe in open source research and you don't even have to tell us! In case you choose to do, we have a vibrant community in Discord! Stop by and say Hi üëã\n",
        "\n",
        "<img src=\"https://github.com/ShishirPatil/gorilla/blob/gh-pages/assets/img/logo.png?raw=true\" width=30% height=30%>"
      ],
      "metadata": {
        "id": "7bKku43frr8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gorilla ü¶ç is hosted by UC Berkeley Sky lab for FREE ü§© as a research prototype ü§ì\n",
        "## Please don't use it for commercial serving üëÄ\n",
        "## The hosted models are only trained to serve HuggingFace/TF/Torch APIs. They are NOT trained to serve other restful APIs."
      ],
      "metadata": {
        "id": "5PA9GQbV4rcN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBd_fso7qFPX"
      },
      "outputs": [],
      "source": [
        "# Import Chat completion template and set-up variables\n",
        "!pip install openai &> /dev/null\n",
        "import openai\n",
        "import urllib.parse\n",
        "\n",
        "openai.api_key = \"EMPTY\" # Key is ignored and does not matter\n",
        "openai.api_base = \"http://zanino.millennium.berkeley.edu:8000/v1\"\n",
        "# Alternate mirrors\n",
        "# openai.api_base = \"http://34.132.127.197:8000/v1\"\n",
        "\n",
        "# Report issues\n",
        "def raise_issue(e, model, prompt):\n",
        "    issue_title = urllib.parse.quote(\"[bug] Hosted Gorilla: <Issue>\")\n",
        "    issue_body = urllib.parse.quote(f\"Exception: {e}\\nFailed model: {model}, for prompt: {prompt}\")\n",
        "    issue_url = f\"https://github.com/ShishirPatil/gorilla/issues/new?assignees=&labels=hosted-gorilla&projects=&template=hosted-gorilla-.md&title={issue_title}&body={issue_body}\"\n",
        "    print(f\"An exception has occurred: {e} \\nPlease raise an issue here: {issue_url}\")\n",
        "\n",
        "# Query Gorilla server\n",
        "def get_gorilla_response(prompt=\"I would like to translate from English to French.\", model=\"gorilla-7b-hf-v1\"):\n",
        "  try:\n",
        "    completion = openai.ChatCompletion.create(\n",
        "      model=model,\n",
        "      messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "  except Exception as e:\n",
        "    raise_issue(e, model, prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßë‚Äçüíª [Update Jun 15] With our new v1 model `gorilla-7b-hf-delta-v1`, Gorilla now returns code snippets you can use directly in your workflow!"
      ],
      "metadata": {
        "id": "FNDBCAMBV0aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1: Translation ‚úç with ü§ó"
      ],
      "metadata": {
        "id": "asdPNq38qIx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gorilla `gorilla-mpt-7b-hf-v1` with code snippets\n",
        "# Translation\n",
        "prompt = \"I would like to translate 'I feel very good today.' from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v1\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2lVWV9yWO0i",
        "outputId": "cff3700b-8a94-494e-c802-7fa903d8539a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Natural Language Processing Translation\n",
            "<<<api_call>>>: translation = pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. We import the pipeline function from the transformers library provided by Hugging Face.\n",
            "2. The pipeline function is used to create a translation model for English to Chinese, which is capable of translating text from one language to another.\n",
            "3. We specify the model 'Helsinki-NLP/opus-mt-en-zh' to be loaded. This is a pre-trained model that has been fine-tuned on a large corpus of text, and it can translate English text into Chinese.\n",
            "4. The created translation pipeline can be used to translate text from English to Chinese.\n",
            "<<<code>>>:\n",
            "from transformers import pipeline\n",
            "\n",
            "def load_model():\n",
            "    translation = pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\n",
            "    return translation\n",
            "\n",
            "def process_data(text, translation):\n",
            "    response = translation(text, max_length=50)[0]['translation_text']\n",
            "    return response\n",
            "\n",
            "text = 'I feel very good today.'\n",
            "\n",
            "# Load the translation model\n",
            "translation = load_model()\n",
            "\n",
            "# Process the data\n",
            "response = process_data(text, translation)\n",
            "\n",
            "print(response)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WsNxThC3mF3",
        "outputId": "e24fd10c-7c90-412e-de6b-8b62bd06d852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[sentencepiece]) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def load_model():\n",
        "    translation = pipeline('translation_en_to_zh', model='Helsinki-NLP/opus-mt-en-zh')\n",
        "    return translation\n",
        "\n",
        "def process_data(text, translation):\n",
        "    response = translation(text, max_length=50)[0]['translation_text']\n",
        "    return response\n",
        "\n",
        "text = 'I feel very good today.'\n",
        "\n",
        "# Load the translation model\n",
        "translation = load_model()\n",
        "\n",
        "# Process the data\n",
        "response = process_data(text, translation)\n",
        "\n",
        "print(response)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otAMKg0J3g4A",
        "outputId": "bbe7a1a7-2a8d-4510-95ce-096cde8f47ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Êàë‰ªäÂ§©ÊÑüËßâÂæàÂ•Ω\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2: Object detection üî∑ with ü§ó"
      ],
      "metadata": {
        "id": "Gnx7YHf18DTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gorilla `gorilla-7b-hf-v1` with code snippets\n",
        "# Object Detection\n",
        "prompt = \"I want to build a robot that can detecting objects in an image ‚Äòcat.jpeg‚Äô. Input: [‚Äòcat.jpeg‚Äô]\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v1\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvQawbSxWoEu",
        "outputId": "9cbb01f9-02ae-4d40-951b-48df221f1c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Computer Vision Object Detection\n",
            "<<<api_call>>>: model = OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. from PIL import Image, and from transformers import OwlViTProcessor, OwlViTForObjectDetection import the necessary components from the Python Image Library and the Hugging Face Transformers library.\n",
            "2. image_path is the path to the image you want to process.\n",
            "3. An instance of the image is opened using Image.open(image_path).\n",
            "4. The model and processor are initialized from the pretrained OwlViT model.\n",
            "5. inputs are generated by processing the image using the processor.\n",
            "6. The model's predictions are generated by passing the inputs to the model.\n",
            "7. The outputs are post-processed, and object labels are retrieved using the predicted_boxes dictionary.\n",
            "<<<code>>>:\n",
            "from PIL import Image\n",
            "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
            "\n",
            "def load_model():\n",
            "    processor = OwlViTProcessor.from_pretrained('google/owlvit-base-patch32')\n",
            "    model = OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32')\n",
            "    return processor, model\n",
            "\n",
            "def process_data(image_path, processor, model):\n",
            "    image = Image.open(image_path)\n",
            "    inputs = processor(images=image, return_tensors='pt')\n",
            "    outputs = model(**inputs)\n",
            "    target_sizes = torch.Tensor([image.size[::-1]])\n",
            "    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n",
            "    labels = results[0]['labels']\n",
            "    return labels\n",
            "\n",
            "image_path = 'cat.jpeg'\n",
            "\n",
            "# Load the model and processor\n",
            "processor, model = load_model()\n",
            "\n",
            "# Process the data\n",
            "labels = process_data(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's try to invoke APIs from Torch Hub instead for the same prompts!"
      ],
      "metadata": {
        "id": "Ot4EKOLXhpoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Translation ‚úç with Torch Hub\n",
        "prompt = \"I would like to translate from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-th-v0\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS-ZeP0rhmzp",
        "outputId": "01e4f245-e99c-4c58-80ce-d9ad9b20f6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'domain': 'Machine Translation', 'api_call': \\\"model = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')\\\", 'api_provider': 'PyTorch', 'explanation': 'Load the Transformer model from PyTorch Hub, which is specifically trained on the WMT 2014 English-French translation task.', 'code': 'import torch\\nmodel = torch.hub.load('pytorch/fairseq', 'transformer.wmt14.en-fr', tokenizer='moses', bpe='subword_nmt')'}\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚õ≥Ô∏è With Gorilla being fine-tuned on MPT, and Falcon, you can use Gorilla commercially with no obligations! üü¢"
      ],
      "metadata": {
        "id": "hvQ3q5AX1wqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gorilla with `gorilla-mpt-7b-hf-v0`\n",
        "prompt = \"I would like to translate from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-mpt-7b-hf-v0\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGi3wwmQ1voP",
        "outputId": "99d7af83-c705-495c-e7b1-445ae5e6c4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide the English text you would like to translate: \\\"Translate this text to Chinese:\\\"\\n<<<domain>>>: Natural Language Processing Text2Text Generation\\n<<<api_call>>>: M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\n<<<api_provider>>>: Hugging Face Transformers\\n<<<explanation>>>: 1. Import the necessary libraries - M2M100ForConditionalGeneration and M2M100Tokenizer from the transformers library.\\n2. Load the pretrained model 'facebook/m2m100_1.2B' and its corresponding tokenizer.\\n3. Set the source language to English (en) and use the tokenizer to tokenize the input text.\\n4. Use the model to generate the translated text in Chinese by providing the tokenized input to the 'generate' function.\\n5. Decode the generated tokens back into a readable text string using the tokenizer.\\n<<<code>>>: from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\\nsrc_text = \\\"Translate this text to Chinese:\\\"\\nsrc_lang = \\\"en\\\"\\nmodel = M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\\ntokenizer = M2M100Tokenizer.from_pretrained('facebook/m2m100_1.2B')\\ntokenized_input = tokenizer(src_text, return_tensors=\\\"pt\\\", src_lang=src_lang)\\ntranslated_tokens = model.generate(**tokenized_input)\\ntranslated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\\n\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We will deprecate the `gorilla-7b-hf-v0` model on July 4 when we will automatically upgrade all v0 model requests to v1. The only changes between v0 and v1 is better code snippets.\n",
        "Below are example prompt-responses for `gorilla-7b-hf-v0` Legacy Model for ü§ó"
      ],
      "metadata": {
        "id": "fIRsh6Ne9f0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I would like to translate from English to Chinese.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v0\" ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMnJzPaN5FlV",
        "outputId": "732abf5c-5c54-498c-c8ba-22d1d7161539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Natural Language Processing Text2Text Generation\n",
            "<<<api_call>>>: M2M100ForConditionalGeneration.from_pretrained('facebook/m2m100_1.2B')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. Import the necessary libraries, which are M2M100ForConditionalGeneration and M2M100Tokenizer from the transformers package.\n",
            "2. Load the pre-trained model 'facebook/m2m100_1.2B' using the M2M100ForConditionalGeneration.from_pretrained() method. This model is designed for machine-to-machine translation tasks.\n",
            "3. Load the tokenizer using the M2M100Tokenizer.from_pretrained() method. This tokenizer is used to prepare the input text for the model and convert the translated output back into human-readable text.\n",
            "4. Define the source text for translation and tokenize it using the tokenizer.\n",
            "5. Use the model to generate the translated text using the source tokens as input.\n",
            "6. Decode the translated text using the tokenizer and print the result.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I want to build a robot that can detect objects in an image.\"\n",
        "print(get_gorilla_response(prompt, model=\"gorilla-7b-hf-v0\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WochUPqf8HLa",
        "outputId": "a25c281f-d3ff-4fcd-d4ac-ff375f613484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<<<domain>>>: Computer Vision Object Detection\n",
            "<<<api_call>>>: YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
            "<<<api_provider>>>: Hugging Face Transformers\n",
            "<<<explanation>>>: 1. We first import the necessary classes from the transformers, PIL, and requests packages. This includes YolosForObjectDetection for the object detection model and Image for processing image data.\n",
            "2. We then use the from_pretrained method of the YolosForObjectDetection class to load the pre-trained model 'hustvl/yolos-tiny'. This model has been trained for object detection tasks, which is exactly what we need for detecting objects in an image.\n",
            "3. We load the image data from a file or a URL, and then use the model to analyze the image and identify the objects within it.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Using gorilla is as easy as calling `get_gorilla_response()` with your prompt! Try out Gorilla, and share your interesting findings in `#showcase` ü§© [Discord](https://discord.gg/3apqwwME)!"
      ],
      "metadata": {
        "id": "XS5Qe6zD8tdX"
      }
    }
  ]
}